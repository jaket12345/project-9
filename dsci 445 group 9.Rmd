---
title: "DSCI 445 GROUP 9 PROJECT: SPOTIFY MUSIC ANALYTICS"
output: html_document
date: '2022-12-09'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(445) 
```


## Overview of Our Goals 

Jay: To look for trends and similarities in tracks that have at least 
one billion streams (on Spotify).

Jake: To look for trends and changes in popular Rap music by 
decade and as a whole.

Jack: Looking for trends in different hip hop genres and seeing if you can predict the popularity of hip hop songs

## Data Collection 

Using Exportify we were able to extract information about songs contained within a playlist. These files contain numerous variables that we were able to use to investigate our questions of interest.

## Variables 

According to Spotify’s Web Application Programming Interface (API) developer guide;  \ 

● Popularity: 0 to 100 scale, attempting to quantify popularity of a song by total plays within the time since it’s release \ 
● Duration (ms): Length of song \ Values from 0-1
● Danceability: “Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.” \ 0-1
● Energy: “Represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.” \ Values from 0-1
● Loudness: “The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.” \  Values from -15-0
● Tempo: “The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, and derives directly from the average beat duration.” \ 0-200
● Valence: “Describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).” \ Values from 0-1
● Genre: Classifies music by it’s genre as determined on Spotify.\ 
● Mode: “Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.” \ 

## DATA SET 1 (Jay): TRENDS IN BILLION - STREAMED TRACKS

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(445) 

library(dplyr) 
library(tidyr) 

library(psych)
library(modeest) 
library(dplyr)  

library("ggplot2")                     
library("GGally") 
library(tidyverse) 
library(tidymodels) 
library(ISLR) 
library(rsample) 
library(gam)
library(car) 
library(caret)
library(leaps) 
library(ggplot2)  
library(corrplot)  
library(factoextra)  
```


Research question - looking for trends in songs with billions of streams  \ 

# DATA PREPARATION 

From spotify, I picked a playlist called “billion hits” that at the time included all songs that have reached 1 billion streams on the platform, and converted the properties of the songs (song name, artist, loudness, tempo, etc) into an excel file using the Exportify website.   

```{r}

billions <- read.csv("billions_club.csv") 

```


```{r, error=FALSE} 

# removing unwanted variables
billions <- select(billions, -c('ï..Spotify.ID', 'Artist.IDs', 'Album.Name', 'Duration..ms.', 'Added.By', 'Key', 'Mode', 'Instrumentalness', 'Time.Signature'))   

# change character variables to factor variables 
billions$Track.Name <- as.factor(billions$Track.Name)
billions$Artist.Name.s. <- as.factor(billions$Artist.Name.s.)

# updating date variables 
billions$Release.Date <- as.Date(billions$Release.Date)
billions$Added.At <- as.Date(billions$Added.At) 

# change loudness variable negative data to positive 
billions$Loudness <- abs(billions$Loudness) 

# removing missing values 
billions <- na.omit(billions) 

# creating duration variable - the time taken for a single song to reach a billion streams 
billions$duration <- billions$Added.At - billions$Release.Date 

# creating main genre variable with one genre 
musicGenres <- billions$Genres
musicGenres <- as.data.frame(musicGenres) 
main_genre <- c() 

main_genre <- musicGenres %>%
  separate(musicGenres,c("Main.Genre"),",")   

MAIN <- main_genre[['Main.Genre']] 
billions$Main.Genre <- MAIN    
  
```

# DATA SUMMARIES 

```{r}

# summary table 
describe(billions) 

# top 5 popular tracks 
billions2 <- billions[order(-billions$Popularity),]  
billions2 <- billions2[1:5,]  
billions2 

# top 5 popular genres 
genre_groups <- billions %>%
  group_by(Main.Genre) %>%
  summarize(n=n()) 
genre_groups 
 
# artist with highest number of billion-streams tracks 
mlv(billions$Artist.Name.s., method = "mfv")  

billions3 <- billions[order(-billions$Popularity),] 
billions3 <- billions3[billions3$Artist.Name.s. == "Ed Sheeran",]
billions3 

## tracks with longest and shortest duration 
a <- billions[which.min(billions$duration),]
b <- billions[which.max(billions$duration),]
df1 <- bind_rows(a,b)    
df1 


# for (37) songs before 2006 (when Spotify was created), their upload date is the same as their release date. i had to create a new variable that includes updated release dates for them   

billions$actualRelease <- billions$Release.Date # creating release date variable 
spotify_date <- as.Date(c("2006-04-23"))  # creating spotify date 

billions4 <- billions[billions$Release.Date < "2006-04-23",] # removing data to be edited
billions4$actualRelease <- spotify_date # updating variable 

billionsss <- rbind(billions[billions$Release.Date > "2006-04-23",],billions4) # creating new full dataframe with updated data  

billionsss$duration <- billionsss$Added.At - billionsss$actualRelease # update duration variable 

# 

afterJul21 <- billionsss[billionsss$Added.At > "2021-07-21",] # removing data before playlist was made  

# actual tracks with longest and shortest duration
a <- afterJul21[which.min(afterJul21$duration),]
b <- afterJul21[which.max(afterJul21$duration),] 
df1 <- bind_rows(a,b)    
df1 

```

From these summaries, we can deduce the following; \ 

● Majority genre – dance pop (80), pop (39), Canadian contemporary r&b (10) \ 
● Artist with highest number of billion-streams tracks – 9 - Ed Sheeran \ 
● Highest popularity – 92 - sweater weather (2013) by The Neighborhood – duration of 3038 days \ 
● Lowest popularity – 44 – bohemian rhapsody remaster (2018) by Queen – duration of 1006 days \ 

Comparing the most popular and least popular song in the data set, we notice that the less popular song took less time to reach a billion streams, and was also released on the platform at a later date. Looking at graphs involving duration, year of release and popularity could help understand this phenomena.  \ 


# EXPLORATORY DATA ANALYSIS 

```{r} 

# create data frame for numerical data to be analyzed   
billions_data <- select(afterJul21, -c('Track.Name','Artist.Name.s.','Genres','Release.Date','Added.At','duration','actualRelease','Main.Genre'))    

## the corr plot 
M<-cor(billions_data)  
corrplot(M, method="number")


## data plots 
plot(billions_data$Popularity, xlab = "song index", ylab = "popularity") 

ggplot() + geom_point(aes(x = actualRelease, y = Popularity, col = Main.Genre), data = billions) + theme(legend.position ="none")   

ggplot() + geom_point(aes(x = actualRelease, y = duration, col = Main.Genre), data = afterJul21) + theme(legend.position ="none")   

```

From the graphs, we can deduce the following; \ 
● Majority of tracks from late 2010s (2015-2020) \  
● Majority of popularity between 75 and 90 \ 
● Older songs have a longer duration \  
● Shortest duration – 82 – Montero (2021) by Lil Nas X – 17 days \  
● longest duration – 80 – chasing cars (2006) by Snow Patrol – 6051 days \  



```{r} 

# validation sets  
train <- sample(nrow(billions_data) * 0.7)
train_set <- billions_data[train, ]
test_set <- billions_data[-train, ] 

## mlr regression 
billions_mlr <- lm(Popularity ~., data = train_set) 
summary(billions_mlr) 
avPlots(billions_mlr) # The points that are labelled in each plot represent the 2 observations with the largest residuals and the 2 observations with the largest partial leverage. 

predictt1 <- predict(billions_mlr, train_set) 
train_mse1 <- mean((train_set$Popularity - predictt1) ^ 2) 
train_mse1   

predictt2 <- predict(billions_mlr, test_set) 
test_mse1 <- mean((test_set$Popularity - predictt2) ^ 2) 
test_mse1  


# forward stepwise subset selection 

step.model <- regsubsets(Popularity ~., data = train_set, nvmax = 8, method = "forward") 
model_summary <- summary(step.model)  
model_summary  

plot(model_summary$adjr2) # results = go with 3 variables

coef(step.model, id = 3) 


## fit selected variables on a GAM # checking non-linear r/ships 
gam_model <- gam(Popularity ~ s(Danceability) + s(Liveness) + s(Tempo), data = train_set) 
summary(gam_model) 
plot(gam_model) 

# training error 
pred <- predict(gam_model, train_set)
train_mse <- mean((train_set$Popularity - pred) ^ 2) 
train_mse 

# evaluating on test set 
preds <- predict(gam_model, test_set) 
test_mse <- mean((test_set$Popularity - preds) ^ 2) 
test_mse 

## UNSUPERVISED LEARNING : k-clustering 
fviz_nbclust(billions_data, kmeans,method = "wss") # choosing k 
km.res <- kmeans(billions_data, 3, iter.max = 8, nstart = 1) 
km.res 

fviz_cluster(km.res,billions_data, ellipse.type = "norm", geom = "point")   


```

Tempo is the only significant variable from both multiple linear regression and the general linearized model. Their graphs tell us that when holding other variables at constant, song popularity increases with the increase in song tempo. \ 
With an attempt at an unsupervised learning model, the k-clustering resulted with the clusters mostly overlapping, telling us that there was really no difference among the 3 groups of songs clustered together. \ 





## DATA SET 2 (Jake): How was Rap changed since it's creation in the 70's?

-[1980-2019 hip hop.csv](https://github.com/dsci445-csu/project-9/files/10241196/1980-2019.hip.hop.csv)
```{r}
# Seeds and Packages Needed
set.seed(445) 
library(dplyr) 
library(tidyr) 
library(psych)
library(modeest) 
library(dplyr)  
library(ggplot2)                     
library(GGally) 
library(tidyverse) 
library(tidymodels) 
library(ISLR) 
library(rsample) 
library(gam)
library(car) 
library(caret)
library(leaps) 
library(ggplot2)  
library(corrplot)  
library(factoextra)
library(mgcv)

# Read In Data Set (Included in Main Project 9 Repository)
top2021 <- read.csv("top_hits_of_2021.csv") 

# Making Decade a Factor / Rename Datset to "Rap"
rap <- read_csv("1980-2019 hip hop.csv",
                col_types = cols(Decade = col_factor(levels = c("80", "90", "00", "10"))))
                
# Only Include Numeric Variables for Modeling
rap_data <- rap[,4:15]
```
# Data Summaries

```{r}
# MLR model
music.lm <- lm(Popularity ~ ., data = rap_data) #modeling all variables against Popularity
# Model Summary
summary(music.lm)
```
Notable Summary Stats
- Significant Variables at $\alpha$ = .05: Duration, Danceability, Energy, Loudness, and Valence
  - Variables to focus on for EDA

# Exploratory Data Analysis (Plots)

```{r}
# Scatterplots investigating relationships to music popularity
ggplot(rap, aes(x = Popularity, y = Danceability)) +
  geom_point() +
  geom_smooth(method = "lm", se = T) +
  theme_classic()

ggplot(rap, aes(x = Popularity, y = Valence, color = Decade)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_classic()

ggplot(rap, aes(x = Popularity, y = Energy, color = Decade)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  theme_classic()
 
# Barplot Showing Tempo over Decades
ggplot(rap, aes(x = Decade, y = Tempo)) +
  geom_bar(stat="identity") +
  ggtitle("Rap Music's Tempo by Decade") +
  theme_classic()
```
Notable trends from EDA plots
- Tempo Increasing over time
- Valence and Energy Decrease as Popularity Increases across most Decades

# GAM Model

```{r}
# split into test and training sets
train <- sample(nrow(rap_data) * 0.75)
train_set <- rap_data[train, ]
test_set <- rap_data[-train, ] 

# subset selection
step.model <- regsubsets(Popularity ~., data = train_set, nvmax = 8, method = "forward") 
model_summary <- summary(step.model)  
model_summary  
plot(model_summary$adjr2)

## fit selected variables to GAM
gam1 <- gam(Popularity ~ s(Valence) + s(Tempo) + s(Danceability), data = train_set) 
summary(gam1) 
plot(gam1) 

# calculate training error 
pred <- predict(gam_model, train_set)
train_mse <- mean((train_set$Popularity - pred) ^ 2) 
train_mse 

# evaluating on test set 
preds <- predict(gam_model, test_set) 
test_mse <- mean((test_set$Popularity - preds) ^ 2) 
test_mse 

```
- 3 Variables Optimal per Subset Selection
- Significant Variables
  - Tempo
  - Valence
- Train MSE = 128.2
- Test MSE = 188.5

## DATA SET 3 (Jack): What are the different trends in the hip hop genres, and can you predict if a hip hop song will be popular?

# Always set seed no matter what
```{r}

set.seed(445)

```
# You need these Datasets downloaded for this code to run: detHH, altHH, atlHH, undHH, conHH, allHH, hiphop
# If you do not have all of those datasets it will not work
# ALL of these data sets are included in hiphop Datasets-JackDeere
# Please download them and have them

# Libraries 
```{r}
library(ggplot2)
library(FactoClass)
library(e1071)
library(glmnet)
library(stats)


```

# This is the function i use to determing the prediction accuracy of the model
```{r}
predAcc <- function(validation, pred, df) {
  SSE <- sum((pred - validation)^2)
  SST <- sum((validation - mean(validation))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))
  
  
  data.frame(
  RMSE = RMSE,
  Rsquare = R_square
  
 
)
  
  }




```





# Making The Data Sets
# There was much much more dataset transformation but I would have to repeat it to show it in the code which would take hours

```{r}


detHH <- read.csv("detHH.csv")#dataset with all of the songs with their main genre being detroit hip hop
atlHH <- read.csv("atlHH.csv") #dataset with all of the songs with their main genre being atlanta hip hop
altHH <- read.csv("altHH.csv") #dataset with all of the songs with their main genre being alternative hip hop
hiphop <- read.csv("hiphop.csv") #dataset with all of the songs with their main genre being hip hop
conHH <- read.csv("conHH.csv") #dataset with all of the songs with their main genre being conscious hip hop
undHH <- read.csv("undHH.csv") #dataset with all of the songs with their main genre being underground hip hop
allHH <- read.csv("allHH.csv") #dataset with all of the songs with their main genre being underground hip hop

detHH <- detHH[,-1]
atlHH <- atlHH[,-1]
altHH <- altHH[,-1]
hiphop <- hiphop[,-1]
conHH <- conHH[,-1]
undHH <- undHH[,-1]
allHH <- allHH[,-1]

allHH <- rbind(allHH,detHH)


```


# Linear Model of hiphop.csv

```{r}
hiphop <- as.data.frame(sapply(hiphop, as.numeric))



hhModel <- lm(Popularity~., data = hiphop)
summary(hhModel)



```


# Graphs that show the stat sig (statistically significant) variables trends

```{r}

ggplot(data = hiphop, aes(x = Popularity, y =Energy)) + geom_point()  + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Energy")

ggplot(data = hiphop, aes(x = Popularity, y =Loudness)) + geom_point()  + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Loudness")


```

# Linear Model of altHH.csv

```{r}
altMod <- lm(Popularity~.,data = altHH)

summary(altMod)



```



# Graphs that show the stat sig (statistically significant) variables trends

```{r}

ggplot(data = altHH, aes(x = Popularity, y =Valence)) + geom_point()  + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Valence")

ggplot(data = altHH, aes(x = Popularity, y =Speechiness)) + geom_point()  + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Speechiness")


```



# Linear Model of conHH.csv

```{r}

conMod <- lm(Popularity~.,data = conHH)

summary(conMod)


```



# Graphs that show the stat sig (statistically significant) variables trends

```{r}

ggplot(data = altHH, aes(x = Popularity, y =Energy)) + geom_point()  + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Energy")

ggplot(data = altHH, aes(x = Popularity, y =Mode)) + geom_point()  + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Mode")


```



# Linear Model of undHH.csv

```{r}


undMod <- lm(Popularity~.,data = undHH)

summary(undMod)



```




# Linear Model of atlHH.csv

```{r}

atlMod <- lm(Popularity~.,data = atlHH)

summary(atlMod)




```



# Graphs that show the stat sig (statistically significant) variables trends

```{r}
ggplot(data = atlHH, aes(x = Popularity, y = Loudness)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Loudness")

ggplot(data = atlHH, aes(x = Popularity, y = Speechiness)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Specchiness")



```




# Linear Model of allHH.csv

```{r}


allHHMod <- lm(Popularity~., data = allHH)

summary(allHHMod)
 

plot(allHHMod)

```



# Graphs that show the stat sig (statistically significant) variables trends

```{r}

ggplot(data = allHH, aes(x = Popularity, y = Danceability)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Danceability")

ggplot(data = allHH, aes(x = Popularity, y = Energy)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Energy")

ggplot(data = allHH, aes(x = Popularity, y =Loudness)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Loudness")


```



# Linear Model of detHH.csv

```{r}

detMod <- lm(Popularity~., detHH)

summary(detMod)



```

# Graphs that show the stat sig (statistically significant) variables trends

```{r}

ggplot(data = detHH, aes(x = Popularity, y = Speechiness)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Speechiness")

ggplot(data = detHH, aes(x = Popularity, y = Acousticness)) + geom_point() + geom_smooth(method='lm', formula= y~x) + ggtitle(" Popularity v Acousticness")



```


Prediction


# Splitting the data and creatign a glm model
```{r}
trainHH <- allHH[1:668,]
testHH <- allHH[668:835,]

hhGLM <- glm(Popularity~., data = trainHH)

summary(hhGLM)


```

# Finding the prediction accuracy of the model

```{r}

predictions <- predict(hhGLM,testHH, interval = "confidence")
predictions


predAcc(testHH$Popularity,predictions, testHH)



```

# 9% prediction accuracy...
# Very bad, cannot use glm for prediction model


# makign a ridge rregression model and testing its pred accuracy

# i commented this out because i got what seems like a unfixable error. tried close to everything and nothing fixed it. the lasso regression is also commented out. It ran one time each before it stopped working, and I was able to get the pred accuracy.
```{r}
#modelTrain=model.matrix (Popularity~.,trainHH)[]
#dim(modelTrain)

#popTrain=trainHH$Popularity


#xtest=model.matrix (Popularity~.,testHH)[]

#ytest=testHH$Popularity



#cv.out=cv.glmnet (modelTrain,popTrain,alpha =0)


#bestlam=cv.out$lambda.min


#model =glmnet(modelTrain,popTrain,alpha=0,lambda=bestlam)


#model$beta


#pred=predict(model,s=bestlam ,newx=xtest)

#dim(ytest)
#dim(pred)
#predAcc(ytest, pred, testHH)

```

# Less than 10% pred accuracy
# Ridge Regression also not good





# Making a lasso regresssion model and testing it for its pred accuracy
```{r}
# modelTrain<-model.matrix (Popularity~.,trainHH)
# 
# popTrain<-trainHH$Popularity
# 
# 
# xtest=model.matrix (Popularity~.,testHH)
# 
# ytest<-testHH$Popularity
# 
# 
# 
# cv.out <-cv.glmnet (modelTrain,popTrain,alpha =1)
# 
# 
# bestlam<-cv.out$lambda.min
# 
# 
# model <- glmnet(modelTrain,popTrain,alpha=1,lambda=bestlam)
# 
# 
# model$beta
# 
# 
# pred <- predict(model,s=bestlam ,newx=xtest)
# 
# 
# predAcc(ytest, pred, testHH)
# 
# 


```
# Less than 10% pred accuracy
# Lasso Regression is also bad



# Making a suport vector machine model and finding the pred accuracy
```{r}


tuneCost <- tune(svm, Popularity~ ., data = trainHH, kernel = "polynomial", ranges = list(cost = 10^seq(-2, 
    1, by = 0.25)))


summary(tuneCost)
svmMod <- svm(Popularity~ Loudness + Energy + Duration..ms. + Danceability, kernel = "radial", data = trainHH,  cost = tuneCost$best.parameters$cost)
predictions <- predict(svmMod, testHH)


predAcc(testHH$Popularity,predictions, testHH)



plot(svmMod)

```

# 5% pred accuracy rate
# Svm also bad. Seems like you cant really predict it 



# code that was used to get rid of all but the main genre in the variable 

```{r}

for(x in 1:2825){ #length of your music dataframes ( number of rows)
  
  #the value of Genres in the origial dataframe is actually just one big character variable, which is why its harder to get just the first genre out
  #musicGenres is the "list" of genres that are in the original dataframe
  #dataframe will be the name of your music dataframe
  #MainGenre is the main genre that it pulls from the musicGenres variable
  
 musicGenres <- dataframe[x,11]  #puts genres into variable. The 11 is the column that the gerne variable is in.
 
 musicGenres <- as.data.frame(musicGenres) #turns it into a dataframe
 
 MainGenre <- musicGenres %>% separate(musicGenres, c("Main Genre"), sep = ',') # grabs first genre in list
  MainGenre
dataframe[x,11] <- MainGenre #sets first genre to genre value in dataframe
}
```


# Summary: 
# If we had more time, further research could have been done on various things:
# Variables influencing rise in streaming 
# Comparing music statistics among different streaming platforms
# Comparing across all mainstream genres
# Comparing music across different languages


# Some problems we ran into while working with the data:
# Trouble finding exact calculations for variables
# Filtering, and combining datasets
# Shaping the data to be used for models

